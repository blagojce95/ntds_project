{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensRecommenderSystem():\n",
    "    ''' This class represents the movie recommender system.\n",
    "        It has all functions needed to read the input data\n",
    "        and construct the graphs, compute the specific properties\n",
    "        of the graphs, train a model with SGD and filter the\n",
    "        learned features with graph signal processing techniques.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def read_data(self, path):\n",
    "        ''' Reads an input data file with a given path.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path: string\n",
    "            Represents tha path to the file.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data: pd.DataFrame\n",
    "            The actual data as pd.DataFrame object.\n",
    "        '''\n",
    "        \n",
    "        # Read the file, get the needed features and rename them in a more convenient way\n",
    "        data = pd.read_csv(path, delimiter = '\\t', header = None).rename(columns = {0: 'user', 1: 'movie', 2: 'rating'})\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def get_similarity_matrix(self, data):\n",
    "        ''' Calculates the cosine similarity between every\n",
    "            row in the 'data'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: np.ndarray\n",
    "            The input data for which the cosine similarity\n",
    "            should be calculated. Every row considered as\n",
    "            a different entity, thus the similarity is\n",
    "            calculated for the rows.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sim: np.ndarray\n",
    "            The cosine similarity matrix of the input data.\n",
    "            If the data has a shape (N, M) then the output\n",
    "            has a shape (N, N).\n",
    "        '''\n",
    "        \n",
    "        # Calculate the dot product between every pair of rows\n",
    "        sim = data.dot(data.T)\n",
    "        \n",
    "        # Calculate the length of each rows in (L2 norm)\n",
    "        lens = np.power(np.matrix.diagonal(sim), 1 / 2)\n",
    "        \n",
    "        # Divide the each similarity entry with the length of the first vector\n",
    "        sim = (sim.T / lens).T\n",
    "        \n",
    "        # Switch components, do the same thing as before -- divide each\n",
    "        # rows with the length of the second vector in the product.\n",
    "        # Then switch back to the original matrix.\n",
    "        sim = sim.T\n",
    "        sim = (sim.T / lens).T\n",
    "        sim = sim.T\n",
    "        \n",
    "        return sim\n",
    "    \n",
    "    \n",
    "    def compute_laplacian(self, adjacency: np.ndarray, normalize: bool = True):\n",
    "        ''' Calculates the graph laplacian matrix for a given adjacency matrix.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        adjacency: np.ndarray\n",
    "            The adjacency matrix of the graph.\n",
    "        \n",
    "        normalize: bool\n",
    "            If True, the normalized graph laplacian matrix will be computed.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        L: np.ndarray\n",
    "            The combinatorial or normalized graph laplacian matrix of the graph.\n",
    "        '''\n",
    "        \n",
    "        # Make sure the adjacency matrix is symmetric\n",
    "        assert (adjacency - adjacency.T).min(axis = 0).min(axis = 0) == 0\n",
    "\n",
    "        # Get the degree and diagonal entries of the adjacency matrix\n",
    "        W = adjacency\n",
    "        deg = W.sum(axis = 0)\n",
    "        D = np.diag(deg)\n",
    "\n",
    "        # Calculate the combinatorial laplacian matrix\n",
    "        L = D - W\n",
    "\n",
    "        # Calculate the normalized laplacian matrix\n",
    "        Dn = np.sqrt(np.linalg.inv(D))\n",
    "        Ln = Dn.dot(L).dot(Dn)\n",
    "\n",
    "        return Ln if normalize else L\n",
    "    \n",
    "    \n",
    "    def spectral_decomposition(self, laplacian: np.ndarray):\n",
    "        ''' Calculates the spectral (eigendecomposition) of a given (graph laplacian) matrix.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        laplacian: np.ndarray\n",
    "            The graph laplacian matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        lamb: np.ndarray\n",
    "            Eigenvalues of the graph laplacian matrix (in sorted ascending order).\n",
    "        \n",
    "        U: np.ndarray\n",
    "            The corresponding eigenvectors.\n",
    "        '''\n",
    "        R = laplacian\n",
    "        lamb, U = np.linalg.eigh(R)    \n",
    "        return lamb, U\n",
    "    \n",
    "    \n",
    "    def number_of_connected_components(self, lamb: np.array, threshold: float = 1e-10):\n",
    "        ''' Calculates the number of connected components in the graph for given\n",
    "            eigenvalues of the graph laplacian matrix.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lamb: np.ndarray\n",
    "            Eigenvalues of the graph laplacian matrix (in sorted ascending order).\n",
    "        \n",
    "        threshold: float\n",
    "            The threshold for which, if less, we consider the eigenvalue to be 0.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        num_connected_components: int\n",
    "            The number of connected components.\n",
    "        '''\n",
    "        return np.sum((lamb < threshold).astype(int))\n",
    "    \n",
    "    \n",
    "    def fit_polynomial(self, lam: np.ndarray, order: int, spectral_response: np.ndarray):\n",
    "        ''' Caclulates the polynomial coefficients of the fitted curve for\n",
    "            a given filter / spectral response.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lamb: np.ndarray\n",
    "            Eigenvalues of the graph laplacian matrix (in sorted ascending order).\n",
    "        \n",
    "        order: int\n",
    "            Specifies the degree of the polynomial to be fitted.\n",
    "        \n",
    "        spectral_response: np.ndarray\n",
    "            Represents the desired filter that we want to approximate.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        coeff: np.ndarray\n",
    "            The coefficients of the approximation.\n",
    "        '''\n",
    "        V = np.vander(lam, order + 1, increasing = True)\n",
    "        return np.linalg.lstsq(V, spectral_response, rcond = None)[0]\n",
    "    \n",
    "    \n",
    "    def polynomial_graph_filter_response(self, coeff: np.array, lam: np.ndarray):\n",
    "        ''' Calculates the actual fitted curve -- the approximation of the filter.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        coeff: np.ndarray\n",
    "            The coefficients of the approximation.\n",
    "        \n",
    "        lamb: np.ndarray\n",
    "            Eigenvalues of the graph laplacian matrix (in sorted ascending order).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        response: np.ndarray\n",
    "            The actual values of the approximated filter.\n",
    "        '''\n",
    "        V = np.vander(lam, coeff.shape[0], increasing = True)\n",
    "        return V.dot(coeff)\n",
    "\n",
    "    \n",
    "    def polynomial_graph_filter(self, coeff: np.array, laplacian: np.ndarray):\n",
    "        ''' Calculates the laplacian polynomial with given coefficients.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        coeff: np.ndarray\n",
    "            The coefficients of the approximation.\n",
    "        \n",
    "        laplacian: np.ndarray\n",
    "            The graph laplacian matrix.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        polynom_graph_filter: np.ndarray\n",
    "            The laplacian polynomial matrix with given coefficients.\n",
    "        '''\n",
    "        return sum(coeff[i] * np.linalg.matrix_power(laplacian, i) for i in range(len(coeff)))\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_data_path, seed = 17):\n",
    "        ''' Initialization of the class. Contains many preprocessing steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data_path: string\n",
    "            Represents the name of the training data file.\n",
    "        \n",
    "        seed: int\n",
    "            Random seed used for reproducibility.\n",
    "        '''\n",
    "        \n",
    "        # Initialize the parameters of the model\n",
    "        self.DATA_DIR = 'ml-100k/'\n",
    "        self.train_data_path = train_data_path\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Read the main input data\n",
    "        self.train_df = self.read_data(os.path.join(self.DATA_DIR, self.train_data_path))\n",
    "        # Read the additional movie data\n",
    "        self.movie_df = pd.read_csv(os.path.join(self.DATA_DIR, 'u.item'), delimiter = '|',\n",
    "                                    header = None, encoding = 'latin1') \\\n",
    "                          .drop(columns = [2, 3, 4]).rename(columns = {0: 'movie', 1: 'title'})\n",
    "        \n",
    "        # Find list of unique user IDs\n",
    "        self.inv_map_users = list(set(self.train_df['user']))\n",
    "        self.num_users = len(self.inv_map_users)\n",
    "\n",
    "        # Find list of unique movie IDs\n",
    "        self.inv_map_movies = list(set(self.train_df['movie']))\n",
    "        self.num_movies = len(self.inv_map_movies)\n",
    "        \n",
    "        # Find mapping of the original user ID to [0, N - 1]\n",
    "        # Useful for case when the user IDs are not indexed in [0, N - 1]\n",
    "        self.map_users = {x: i for i, x in enumerate(self.inv_map_users)}\n",
    "        self.map_movies = {x: i for i, x in enumerate(self.inv_map_movies)}\n",
    "        \n",
    "        # Apply the mapping of the IDs found above\n",
    "        self.train_df['user'] = self.train_df['user'].apply(lambda x: self.map_users[x])\n",
    "        self.train_df['movie'] = self.train_df['movie'].apply(lambda x: self.map_movies[x])\n",
    "        self.movie_df['movie'] = self.movie_df['movie'].apply(lambda x: self.map_movies[x])\n",
    "        \n",
    "        # Compute mean and standard deviation for the ratings of the users and movies\n",
    "        self.user_mean = self.train_df.groupby('user')['rating'].mean()\n",
    "        self.user_std = self.train_df.groupby('user')['rating'].std()\n",
    "        self.movie_mean = self.train_df.groupby('movie')['rating'].mean()\n",
    "        self.movie_std = self.train_df.groupby('movie')['rating'].std()\n",
    "        \n",
    "        # Create users graph\n",
    "        self.user_repr = np.zeros((self.num_users, self.num_movies))\n",
    "        # Get the 'net' rating of each user -- subtract the mean of that user\n",
    "        for u, i, r in zip(self.train_df['user'], self.train_df['movie'], self.train_df['rating']):\n",
    "            self.user_repr[u, i] = (r - self.user_mean[u]) / self.user_std[u] # or do not really have to add / std, result is the same\n",
    "        # Calculate cosine simlarity between these representations of the users\n",
    "        self.user_similarity = self.get_similarity_matrix(self.user_repr)\n",
    "        # Fix the similarity matrix so that each user is not connected with himself\n",
    "        self.user_similarity -= np.identity(self.num_users)\n",
    "        \n",
    "        # Define some constat c_users such that only the similarities smaller than\n",
    "        # c_users * mean will be converted into edges in the graph.\n",
    "        c_users = 5.25\n",
    "        user_mean_similarity = self.user_similarity.mean()\n",
    "        self.user_adj = (self.user_similarity > c_users * user_mean_similarity).astype(int)\n",
    "        \n",
    "        # Calculate the normalized graph laplacian matrix\n",
    "        self.user_laplacian_norm = self.compute_laplacian(self.user_adj)\n",
    "        # Find its eigenvalues and eigenvectors\n",
    "        self.user_lamb_norm, self.user_U_norm = self.spectral_decomposition(self.user_laplacian_norm)\n",
    "#         print('Number of connected components on users graph: %d' % self.number_of_connected_components(self.user_lamb_norm))\n",
    "        \n",
    "#         plt.spy(self.user_adj)\n",
    "#         plt.show()\n",
    "        \n",
    "        # Create a desired filter for the users\n",
    "        self.user_ideal_filter = np.ones(self.num_users)\n",
    "        self.user_ideal_filter[self.user_lamb_norm >= 0.7] = 0\n",
    "        \n",
    "        # Initialize the degree of polynomial approximation\n",
    "        self.user_order = 15\n",
    "        # Find the approximated coefficients\n",
    "        self.user_coeff = self.fit_polynomial(self.user_lamb_norm, self.user_order, self.user_ideal_filter)\n",
    "        # Find the approximated values of the filter\n",
    "        self.user_graph_filter = self.polynomial_graph_filter(self.user_coeff, self.user_laplacian_norm)\n",
    "\n",
    "#         plt.plot(self.user_lamb_norm, user_ideal_filter)\n",
    "#         plt.plot(self.user_lamb_norm, self.polynomial_graph_filter_response(user_coeff, self.user_lamb_norm))\n",
    "#         plt.legend(['Ideal', 'Polynomial'])\n",
    "#         plt.xlabel('$\\lambda$')\n",
    "#         plt.ylabel('Spectral response')\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "        # Create movies graph\n",
    "        self.movie_adj = np.zeros((self.num_movies, self.num_movies))\n",
    "        # Sort the movies with respect to their mean rating\n",
    "        sort_idx = self.movie_mean.to_numpy().argsort()\n",
    "        # Connect the neighboring movies according to the mean rating\n",
    "        # Makes a trivial tree with diameter M where M is the number of movies\n",
    "        # I.e., m_1 - m_2 - ... - m_M-1 - m_M.\n",
    "        for i, j in zip(sort_idx[1:], sort_idx[:-1]):\n",
    "            self.movie_adj[i, j] = 1\n",
    "            self.movie_adj[j, i] = 1\n",
    "        \n",
    "        # Define some constat c_movies such that the pairs of movies\n",
    "        # whose mean rating differs with less than c_movies will be\n",
    "        # converted into edges. It was chosen such that the graph\n",
    "        # is connected, and is sparse enoguh.\n",
    "        c_movies = 0.20\n",
    "        for i in range(self.num_movies):\n",
    "            for j in range(i, self.num_movies):\n",
    "                if self.movie_adj[i, j] == 0:\n",
    "                    edge = 1 if abs(self.movie_mean[i] - self.movie_mean[j]) < c_movies else 0\n",
    "                    self.movie_adj[i, j] = edge\n",
    "                    self.movie_adj[j, i] = edge\n",
    "        \n",
    "        # Calculate the normalized graph laplacian matrix\n",
    "        self.movie_laplacian_norm = self.compute_laplacian(self.movie_adj)\n",
    "        # Find its eigenvalues and eigenvectors\n",
    "        self.movie_lamb_norm, self.movie_U_norm = self.spectral_decomposition(self.movie_laplacian_norm)\n",
    "#         print('Number of connected components on movies graph: %d' % self.number_of_connected_components(self.movie_lamb_norm))\n",
    "\n",
    "#         plt.spy(self.movie_adj)\n",
    "#         plt.show()\n",
    "        \n",
    "        # Create a desired filter for the movies\n",
    "        self.movie_ideal_filter = np.ones(self.num_movies)\n",
    "        self.movie_ideal_filter[self.movie_lamb_norm >= 0.35] = 0\n",
    "\n",
    "        # Initialize the degree of polynomial approximation\n",
    "        self.movie_order = 15\n",
    "        # Find the approximated coefficients\n",
    "        self.movie_coeff = self.fit_polynomial(self.movie_lamb_norm, self.movie_order, self.movie_ideal_filter)\n",
    "        # Find the approximated values of the filter\n",
    "        self.movie_graph_filter = self.polynomial_graph_filter(self.movie_coeff, self.movie_laplacian_norm)\n",
    "\n",
    "#         plt.plot(self.movie_lamb_norm, self.movie_ideal_filter)\n",
    "#         plt.plot(self.movie_lamb_norm, self.polynomial_graph_filter_response(self.movie_coeff, self.movie_lamb_norm))\n",
    "#         plt.legend(['Ideal', 'Polynomial'])\n",
    "#         plt.xlabel('$\\lambda$')\n",
    "#         plt.ylabel('Spectral response')\n",
    "#         plt.show()\n",
    "        \n",
    "    \n",
    "    def predict(self, P, Q, u, i):\n",
    "        ''' Given the factorized matrices P, Q, ID of the user and movie,\n",
    "            calculate the prediction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        P: np.ndarray\n",
    "            The factorized -- embedding matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized -- embedding matrix of the movies.\n",
    "        \n",
    "        u: int\n",
    "            The ID of the user.\n",
    "        \n",
    "        i: int\n",
    "            The ID of the movie.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        r_pred: float\n",
    "            The prediction for the user 'u' and movie 'i'.\n",
    "        '''\n",
    "        # Predict the rating of user u and movie i\n",
    "        return P[u].dot(Q[i])\n",
    "    \n",
    "    \n",
    "    def cross_validate(self, n_splits = 5, K = 50, learning_rate = .01, regularization_rate = .017, bias = True, seed = 17):\n",
    "        ''' Evaluate the RMSE of the hyperparameters with cross-validation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_splits: int\n",
    "            The number of folds in the cross-validation.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        seed: int\n",
    "            Used for reproducibility.\n",
    "        '''\n",
    "        # Initialize KFold object\n",
    "        kf = KFold(n_splits = 5, random_state = seed, shuffle = True)\n",
    "        \n",
    "        # Evaluate RMSE for every fold\n",
    "        total_error = 0\n",
    "        for it, (train_idx, val_idx) in enumerate(kf.split(self.train_df)):\n",
    "            # Get the train and validation data\n",
    "            train_data = self.train_df.loc[train_idx]\n",
    "            val_data = self.train_df.loc[val_idx]\n",
    "            \n",
    "            # Get the RMSE for the it-th fold\n",
    "            print('Fold: %d/%d' % (it + 1, n_splits))\n",
    "            cur_error, P, Q = self.evaluate_matrix_factorization(train_data, val_data, K = K, learning_rate = learning_rate,\n",
    "                                                           bias = bias, learn_max_it = 50, filter_max_it = 10,\n",
    "                                                           regularization_rate = regularization_rate, seed = seed)\n",
    "            print()\n",
    "            \n",
    "            # Get the total error (squared errors)\n",
    "            total_error += np.power(cur_error, 2) * len(val_data)\n",
    "        \n",
    "        # Calculate the total RMSE for all n-folds (weighted, useful if the validation datasets have different sizes)\n",
    "        final_error = np.power(total_error / len(self.train_df), 1 / 2)\n",
    "        print('Error: %.5f' % final_error)\n",
    "    \n",
    "    \n",
    "    def learning_step(self, train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias):\n",
    "        ''' Perform one SGD step on the data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The updated factorized matrix for users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The updated factorized matrix for movies.\n",
    "        '''\n",
    "        # For every entry in the training data\n",
    "        for u, i, r in zip(train_data['user'], train_data['movie'], train_data['rating']):\n",
    "            # Get the prediction, and error\n",
    "            r_pred = self.predict(P, Q, u, i)\n",
    "            e_ui = r - r_pred\n",
    "\n",
    "            # Find the updates for the approriate rows in P and Q\n",
    "            nP_u = P[u] + learning_rate * (e_ui * Q[i] - regularization_rate * P[u])\n",
    "            nQ_i = Q[i] + learning_rate * (e_ui * P[u] - regularization_rate * Q[i])\n",
    "\n",
    "            # Set the approriate element to be 1 for the bias\n",
    "            if bias:\n",
    "                nP_u[K] = 1\n",
    "                nQ_i[K + 1] = 1\n",
    "\n",
    "            # Update the matrices\n",
    "            P[u] = nP_u\n",
    "            Q[i] = nQ_i\n",
    "\n",
    "        # Calculate RMSE on the validation dataset\n",
    "        error = 0\n",
    "        for u, i, r in zip(val_data['user'], val_data['movie'], val_data['rating']):\n",
    "            # Get the prediction, and error\n",
    "            r_pred = self.predict(P, Q, u, i)\n",
    "            e_ui = r - r_pred\n",
    "\n",
    "            # Error squared\n",
    "            error += np.power(e_ui, 2)\n",
    "        # Final RMSE\n",
    "        error = np.power(error / len(val_data), 1 / 2)\n",
    "        \n",
    "        return error, P, Q\n",
    "    \n",
    "    \n",
    "    def learn(self, train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, max_it):\n",
    "        ''' Perform at most 'max_it' SGD iterations, or until the RMSE on the validation data set increases.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        max_it: int\n",
    "            The number of maximum SGD iterations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized matrix of users that gives the best RMSE on the validation data set.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized matrix of movies that gives the best RMSE on the validation data set.\n",
    "        '''\n",
    "        error = 1e9\n",
    "        for it in range(max_it):\n",
    "            # Copy the current factorized matrices\n",
    "            nP, nQ = P.copy(), Q.copy()\n",
    "            # Perform one SGD step\n",
    "            cur_error, nP, nQ = x.learning_step(train_data, val_data, nP, nQ, K, learning_rate, regularization_rate, bias)\n",
    "            # print('Learn  it: %3d\\t%.5f' % (it, cur_error))\n",
    "            \n",
    "            # If RMSE on validation dataset increases, stop the learning\n",
    "            if cur_error >= error:\n",
    "                break\n",
    "            \n",
    "            # Else update the error and matrices\n",
    "            error = cur_error\n",
    "            P, Q = nP.copy(), nQ.copy()\n",
    "        \n",
    "        # print('-------------------------')\n",
    "        # print('Final learn it: %3d\\t%.5f\\n' % (it, error))\n",
    "        \n",
    "        return error, P, Q\n",
    "    \n",
    "    \n",
    "    def gsp_learn(self, graph_type, train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, gsp, learn_max_it, filter_max_it):\n",
    "        ''' A wrapper function for learning the factorized matrices if no graph signal processing (GSP) is specified.\n",
    "            If GSP is specified, then graph_type should contain strings representing that the user or movies\n",
    "            graphs should be used to denoise the features in P and Q respectively.\n",
    "            The learning is as follows. Get inital embeddings. Try to apply the specified filter(s) for 'filter_max_it' times, or\n",
    "            until RMSE on the validation dataset decreases. After each filter application, perform SGD steps on the denoised matrices P and Q.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        graph_type: list of strings\n",
    "            Represents the type(s) of graphs that will be used for filtering the features.\n",
    "            If 'USERS' is in the list, then the user matrix P will be denoised with the users graph.\n",
    "            If 'MOVIES' is in the list, then the movies matrix Q will be denoised with the movies graph. \n",
    "        \n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        gsp: bool\n",
    "            If True, graph signal processing will be applied to the features,\n",
    "            according to the list 'graph_type'.\n",
    "        \n",
    "        max_it: int\n",
    "            The number of maximum SGD iterations.\n",
    "        \n",
    "        filter_max_it: int\n",
    "            The number of maximum times the filters should be applied to the\n",
    "            features of the matrices P and Q.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized matrix of users that gives the best RMSE on the validation data set.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized matrix of movies that gives the best RMSE on the validation data set.\n",
    "        '''\n",
    "        # Get the initial factorized matrices without before any filters are applied.\n",
    "        error, P, Q = self.learn(train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it)\n",
    "        \n",
    "        if gsp:\n",
    "            for it in range(filter_max_it):\n",
    "                nP, nQ = P.copy(), Q.copy()\n",
    "                \n",
    "                if 'USER' in graph_type:\n",
    "                    nP[:, :K] = self.user_graph_filter.dot(nP[:, :K])\n",
    "                \n",
    "                if 'MOVIE' in graph_type:\n",
    "                    nQ[:, :K] = self.movie_graph_filter.dot(nQ[:, :K])\n",
    "                \n",
    "                cur_error, nP, nQ = self.learn(train_data, val_data, nP, nQ, K, learning_rate, regularization_rate, bias, learn_max_it)\n",
    "\n",
    "                # print('gsp_learn it: %3d\\t%.5f\\n\\n' % (it, cur_error))\n",
    "                if cur_error >= error:\n",
    "                    break\n",
    "\n",
    "                error = cur_error\n",
    "                P, Q = nP.copy(), nQ.copy()\n",
    "            \n",
    "            # print('-------------------')\n",
    "            # print('Final gsp_learn: %3d\\t%.5f\\n' % (it, error))\n",
    "        \n",
    "        return error, P, Q\n",
    "    \n",
    "    \n",
    "    def movieuser_learn(self, train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it, filter_max_it):\n",
    "        ''' A wrapper function for learning the factorized matrices if graph signal processing (GSP) is specified,\n",
    "            such that both matrices P and Q are updated at the same time.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        max_it: int\n",
    "            The number of maximum SGD iterations.\n",
    "        \n",
    "        filter_max_it: int\n",
    "            The number of maximum times the filters should be applied to the\n",
    "            features of the matrices P and Q.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized matrix of users that gives the best RMSE on the validation data set.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized matrix of movies that gives the best RMSE on the validation data set.\n",
    "        '''\n",
    "        return self.gsp_learn(['USER', 'MOVIE'], train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, True, learn_max_it, filter_max_it)\n",
    "    \n",
    "    \n",
    "    def movie_user_learn(self, train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it, filter_max_it):\n",
    "        ''' Evaluates the RMSE in the following way. First, try to apply the users filter 'filter_max_it' times,\n",
    "            or until the RMSE on the validation set increases. Then, apply the movies fillter once and keep applying\n",
    "            the users filter, as described previously.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        max_it: int\n",
    "            The number of maximum SGD iterations.\n",
    "        \n",
    "        filter_max_it: int\n",
    "            The number of maximum times the filters should be applied to the\n",
    "            features of the matrices P and Q.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized matrix of users that gives the best RMSE on the validation data set.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized matrix of movies that gives the best RMSE on the validation data set.\n",
    "        '''\n",
    "        # Get the best inital embeddings with users filter\n",
    "        error, P, Q = self.gsp_learn(['USER'], train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, True, learn_max_it, filter_max_it)\n",
    "        \n",
    "        for it in range(filter_max_it):\n",
    "            # Copy the best matrices so far\n",
    "            nP, nQ = P.copy(), Q.copy()\n",
    "            \n",
    "            # Apply the movies filter\n",
    "            nQ[:, :K] = self.movie_graph_filter.dot(nQ[:, :K])\n",
    "            # To new SGD steps and users filter application for max_it times or until RMSE on validation decreases\n",
    "            cur_error, nP, nQ = self.gsp_learn(['USER'], train_data, val_data, nP, nQ, K, learning_rate / 10, regularization_rate, bias, True, learn_max_it, filter_max_it)\n",
    "            # print('movie_user it: %3d\\t%.5f\\n\\n' % (it, cur_error))\n",
    "            \n",
    "            # If validation RMSE increases, stop the learning\n",
    "            if cur_error >= error:\n",
    "                break\n",
    "            \n",
    "            # Update the error and matrices P and Q\n",
    "            error = cur_error\n",
    "            P, Q = nP.copy(), nQ.copy()\n",
    "            \n",
    "        # print('-------------------')\n",
    "        print('Final movie_user: %3d\\t%.5f\\n' % (it, error))\n",
    "        return error, P, Q\n",
    "    \n",
    "    \n",
    "    def user_movie_learn(self, train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it, filter_max_it):\n",
    "        ''' Evaluates the RMSE in the following way. First, try to apply the movies filter 'filter_max_it' times,\n",
    "            or until the RMSE on the validation set increases. Then, apply the users fillter once and keep applying\n",
    "            the movies filter, as described previously.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        max_it: int\n",
    "            The number of maximum SGD iterations.\n",
    "        \n",
    "        filter_max_it: int\n",
    "            The number of maximum times the filters should be applied to the\n",
    "            features of the matrices P and Q.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized matrix of users that gives the best RMSE on the validation data set.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized matrix of movies that gives the best RMSE on the validation data set.\n",
    "        '''\n",
    "        # Get the best inital embeddings with movies filter\n",
    "        error, P, Q = self.gsp_learn(['MOVIE'], train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, True, learn_max_it, filter_max_it)\n",
    "        \n",
    "        for it in range(filter_max_it):\n",
    "            # Copy the best matrices so far\n",
    "            nP, nQ = P.copy(), Q.copy()\n",
    "            \n",
    "            # Apply the movies filter\n",
    "            nP[:, :K] = self.user_graph_filter.dot(nP[:, :K])\n",
    "            # To new SGD steps and users filter application for max_it times or until RMSE on validation decreases\n",
    "            cur_error, nP, nQ = self.gsp_learn(['MOVIE'], train_data, val_data, nP, nQ, K, learning_rate / 10, regularization_rate, bias, True, learn_max_it, filter_max_it)\n",
    "            # print('user_movie it: %3d\\t%.5f\\n\\n' % (it, cur_error))\n",
    "            \n",
    "            # If validation RMSE increases, stop the learning\n",
    "            if cur_error >= error:\n",
    "                break\n",
    "            \n",
    "            # Update the error and matrices P and Q\n",
    "            error = cur_error\n",
    "            P, Q = nP.copy(), nQ.copy()\n",
    "            \n",
    "        # print('-------------------')\n",
    "        print('Final user_movie: %3d\\t%.5f\\n' % (it, error))\n",
    "        return error, P, Q\n",
    "\n",
    "    \n",
    "    def evaluate_matrix_factorization(self, train_data, val_data, P = None, Q = None, K = 50, learning_rate = .01, regularization_rate = .017, bias = True, learn_max_it = 50, filter_max_it = 10, seed = 17):\n",
    "        ''' Evaluates the RMSE of the hyperparameters with the desired technique of filter applications.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_data: pd.DataFrame\n",
    "            Represents the training data set.\n",
    "        \n",
    "        val_data: pd.DataFrame\n",
    "            Represents the validation data set.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized (embedding) matrix of the users.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized (embedding) matrix of the movies.\n",
    "        \n",
    "        K: int\n",
    "            The number of latent dimensions for the factorized matrices.\n",
    "        \n",
    "        learning_rate: float\n",
    "            Specifies the step size / learning rate of the SGD update.\n",
    "        \n",
    "        regularization_rate: float\n",
    "            Specifies the regularization rate of the SGD update.\n",
    "        \n",
    "        bias: bool\n",
    "            If True, train a model that takes into consideration\n",
    "            the bias of the users and movies.\n",
    "        \n",
    "        learn_max_it: int\n",
    "            The number of maximum SGD iterations.\n",
    "        \n",
    "        filter_max_it: int\n",
    "            The number of maximum times the filters should be applied to the\n",
    "            features of the matrices P and Q.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        error: float\n",
    "            RMSE error on the validation data set after the SGD updates.\n",
    "        \n",
    "        P: np.ndarray\n",
    "            The factorized matrix of users that gives the best RMSE on the validation data set.\n",
    "        \n",
    "        Q: np.ndarray\n",
    "            The factorized matrix of movies that gives the best RMSE on the validation data set.\n",
    "        '''\n",
    "        # Set the random seed\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Latent space dimension (+ 2 if we use the 'bias' trick with users' and movies' mean ratings)\n",
    "        dim = K + (2 if bias else 0)\n",
    "        \n",
    "        # If P is not given, randomly intialize it.\n",
    "        if P is None:\n",
    "            P = np.random.choice([-0.01, 0.01], size = self.num_users * dim).reshape((self.num_users, dim))\n",
    "            \n",
    "            # Initialize one column to be 1 so that we can learn the bias\n",
    "            if bias:\n",
    "                P[:, K] = 1\n",
    "        \n",
    "        # If Q is not given, randomly initialize it.\n",
    "        if Q is None:\n",
    "            Q = np.random.choice([-0.01, 0.01], size = self.num_movies * dim).reshape((self.num_movies, dim))\n",
    "        \n",
    "            # Initialize one column to be 1 so that we can learn the bias\n",
    "            if bias:\n",
    "                Q[:, K + 1] = 1\n",
    "        \n",
    "        # The standard matrix factorization (MF) model with the bias trick and no GSP\n",
    "        # return self.gsp_learn([], train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, False, learn_max_it, filter_max_it)\n",
    "        \n",
    "        # The MF model when the users and movies filters are applied at the same time\n",
    "        return self.movieuser_learn(train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it, filter_max_it)\n",
    "        \n",
    "        # The MF model when the users filter is applied as many times as it improves RMSE (or filter_max_it), then movies filter once\n",
    "        # return self.movie_user_learn(train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it, filter_max_it)\n",
    "        \n",
    "        # The MF model when the movies filter is applied as many times as it improves RMSE (or filter_max_it), then users filter once\n",
    "        # return self.user_movie_learn(train_data, val_data, P, Q, K, learning_rate, regularization_rate, bias, learn_max_it, filter_max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MovieLensRecommenderSystem('u.data')\n",
    "#error, P, Q = x.evaluate_matrix_factorization(train, test, gsp = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/5\n",
      "\n",
      "Fold: 2/5\n",
      "\n",
      "Fold: 3/5\n",
      "\n",
      "Fold: 4/5\n",
      "\n",
      "Fold: 5/5\n",
      "\n",
      "Error: 0.90614\n"
     ]
    }
   ],
   "source": [
    "x.cross_validate(learning_rate = .01, regularization_rate = .051, bias = True, seed = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/5\n",
      "\n",
      "Fold: 2/5\n",
      "\n",
      "Fold: 3/5\n",
      "\n",
      "Fold: 4/5\n",
      "\n",
      "Fold: 5/5\n",
      "\n",
      "Error: 0.88690\n"
     ]
    }
   ],
   "source": [
    "x.cross_validate(learning_rate = .01, regularization_rate = .051, bias = True, seed = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/5\n",
      "Final movie_user:   8\t0.87527\n",
      "\n",
      "\n",
      "Fold: 2/5\n",
      "Final movie_user:   7\t0.88020\n",
      "\n",
      "\n",
      "Fold: 3/5\n",
      "Final movie_user:   8\t0.87750\n",
      "\n",
      "\n",
      "Fold: 4/5\n",
      "Final movie_user:   7\t0.88400\n",
      "\n",
      "\n",
      "Fold: 5/5\n",
      "Final movie_user:   6\t0.87377\n",
      "\n",
      "\n",
      "Error: 0.87815\n"
     ]
    }
   ],
   "source": [
    "x.cross_validate(learning_rate = .017, regularization_rate = .051, bias = True, seed = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/5\n",
      "\n",
      "Fold: 2/5\n",
      "\n",
      "Fold: 3/5\n",
      "\n",
      "Fold: 4/5\n",
      "\n",
      "Fold: 5/5\n",
      "\n",
      "Error: 0.89069\n"
     ]
    }
   ],
   "source": [
    "x.cross_validate(learning_rate = .017, regularization_rate = .051, bias = True, seed = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
